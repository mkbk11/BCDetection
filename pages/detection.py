import streamlit as st
from PIL import Image
import cv2
import numpy as np
from ultralytics import YOLO
import io
import tempfile
import os
import glob
import pandas as pd
import altair as alt
import zipfile
import requests

# --- Ollama API é…ç½® ---
OLLAMA_BASE_URL = "http://127.0.0.1:11434"
GENERATE_ENDPOINT = "/api/generate"
OLLAMA_MODEL_NAME = "deepseek-r1:7b" # æ‚¨æŒ‡å®šçš„æ¨¡å‹åç§°ï¼Œç¡®ä¿ Ollama æœåŠ¡ä¸­å·²æ‹‰å–æ­¤æ¨¡å‹
# -------------------------

st.set_page_config(page_title="ç»†èƒæ£€æµ‹ - BCDetection-X", layout="wide")

st.title("ğŸ”¬ ç»†èƒæ£€æµ‹")

st.write("è¯·åœ¨ä¾§è¾¹æ é…ç½®æ¨¡å‹å’Œå‚æ•°ï¼Œç„¶åé€‰æ‹©è¾“å…¥æ–¹å¼è¿›è¡Œæ£€æµ‹ã€‚")

# --- é…ç½®ä¾§è¾¹æ  ---
st.sidebar.header("âš™ï¸ é…ç½®é€‰é¡¹")
input_method = st.sidebar.radio("é€‰æ‹©è¾“å…¥æº:", ('ä¸Šä¼ å›¾åƒ', 'ä¸Šä¼ è§†é¢‘', 'ä½¿ç”¨æ‘„åƒå¤´'))

# --- æ¨¡å‹å’Œå‚æ•°é…ç½® ---
st.sidebar.subheader("æ¨¡å‹è®¾ç½®")
# Find .pt files in the 'models' subdirectory relative to the main app directory
# Streamlit pages run from the root directory
models_dir = "models"
model_files_full_path = glob.glob(os.path.join(models_dir, "*.pt"))

# Extract just the filenames for display in the selectbox, but keep full paths for loading
model_display_names = [os.path.basename(f) for f in model_files_full_path]

if not model_files_full_path:
    st.sidebar.warning(f"åœ¨ '{models_dir}' ç›®å½•ä¸­æœªæ‰¾åˆ° .pt æ¨¡å‹æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è·¯å¾„ã€‚")
    selected_model_path = None
    model_display_names = ["æ— å¯ç”¨æ¨¡å‹"]
    default_model_index = 0
else:
    default_model_name = 'BCdetection_YOLOv8.pt'
    default_model_index = 0
    if default_model_name in model_display_names:
        default_model_index = model_display_names.index(default_model_name)

    selected_display_name = st.sidebar.selectbox("é€‰æ‹© YOLO æ¨¡å‹:", model_display_names, index=default_model_index)
    selected_model_path = None
    for full_path in model_files_full_path:
        if os.path.basename(full_path) == selected_display_name:
            selected_model_path = full_path
            break

st.sidebar.subheader("æ¨ç†å‚æ•°")
confidence_threshold = st.sidebar.slider("ç½®ä¿¡åº¦é˜ˆå€¼ (Confidence)", 0.0, 1.0, 0.25, 0.05)
iou_threshold = st.sidebar.slider("äº¤å¹¶æ¯”é˜ˆå€¼ (IoU)", 0.0, 1.0, 0.45, 0.05)
# ----------------------

# --- æ¨¡å‹åŠ è½½ ---
@st.cache_resource
def load_model(model_path):
    try:
        model = YOLO(model_path)
        return model
    except Exception as e:
        st.sidebar.error(f"åŠ è½½æ¨¡å‹ '{os.path.basename(model_path)}' å¤±è´¥: {e}")
        return None

model = None
if selected_model_path:
    model = load_model(selected_model_path)
    if model:
        st.sidebar.info(f"å½“å‰æ¨¡å‹: {os.path.basename(selected_model_path)}")
    else:
        st.sidebar.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æˆ–é€‰æ‹©å…¶ä»–æ¨¡å‹ã€‚")
elif not model_files_full_path:
     st.sidebar.error("æ— å¯ç”¨æ¨¡å‹åŠ è½½ã€‚")
else:
     st.sidebar.error("æœªé€‰æ‹©æœ‰æ•ˆçš„æ¨¡å‹è·¯å¾„ã€‚")

# --- ä¸»å†…å®¹åŒºåŸŸ ---
# col1, col2 = st.columns(2)

# --- å¥åº·é£é™©æç¤ºå‡½æ•° (å¢å¼ºç‰ˆ) ---
def provide_health_tips(counts):
    tips = []
    report_lines = ["--- BCDetection-X åˆ†ææŠ¥å‘Š ---", ""]
    rbc_count = counts.get('RBC', 0)
    wbc_count = counts.get('WBC', 0)
    platelets_count = counts.get('Platelets', 0)
    total_cells = sum(counts.values())

    report_lines.append("**æ£€æµ‹åˆ°çš„ç»†èƒè®¡æ•°:**")
    report_lines.append(f"- çº¢ç»†èƒ (RBC): {rbc_count}")
    report_lines.append(f"- ç™½ç»†èƒ (WBC): {wbc_count}")
    report_lines.append(f"- è¡€å°æ¿ (Platelets): {platelets_count}")
    report_lines.append(f"- æ€»ç»†èƒæ•°: {total_cells}")
    report_lines.append("")

    tips.append("**åˆæ­¥åˆ†æè¯´æ˜:**")
    report_lines.append("**åˆæ­¥åˆ†æè¯´æ˜:**")

    if total_cells == 0:
        tip = "æœªåœ¨å½“å‰è§†é‡æ£€æµ‹åˆ°ç»†èƒã€‚è¯·ç¡®ä¿å›¾åƒæ¸…æ™°ä¸”å…·æœ‰ä»£è¡¨æ€§ã€‚"
        tips.append(f"- {tip}")
        report_lines.append(f"- {tip}")
    else:
        if rbc_count > 0:
            tip = f"æ£€æµ‹åˆ°çº¢ç»†èƒ ({rbc_count} ä¸ª)ã€‚çº¢ç»†èƒè´Ÿè´£è¿è¾“æ°§æ°”ã€‚æ•°é‡å¼‚å¸¸ï¼ˆè¿‡é«˜æˆ–è¿‡ä½ï¼‰å¯èƒ½ä¸å¤šç§å¥åº·çŠ¶å†µæœ‰å…³ï¼Œå¦‚è´«è¡€æˆ–çº¢ç»†èƒå¢å¤šç—‡ã€‚"
            tips.append(f"- {tip}")
            report_lines.append(f"- {tip}")
        if wbc_count > 0:
            tip = f"æ£€æµ‹åˆ°ç™½ç»†èƒ ({wbc_count} ä¸ª)ã€‚ç™½ç»†èƒæ˜¯å…ç–«ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æ•°é‡å‡é«˜å¯èƒ½æç¤ºæ„ŸæŸ“æˆ–ç‚ç—‡ï¼Œé™ä½åˆ™å¯èƒ½å½±å“å…ç–«åŠ›çš„ã€‚ä¸åŒç±»å‹çš„ç™½ç»†èƒï¼ˆå—œä¸­æ€§ç²’ç»†èƒã€æ·‹å·´ç»†èƒç­‰ï¼‰æ¯”ä¾‹ä¹Ÿå¾ˆé‡è¦ï¼Œæœ¬ç³»ç»Ÿæœªåšç»†åˆ†ã€‚"
            tips.append(f"- {tip}")
            report_lines.append(f"- {tip}")
        if platelets_count > 0:
            tip = f"æ£€æµ‹åˆ°è¡€å°æ¿ ({platelets_count} ä¸ª)ã€‚è¡€å°æ¿åœ¨æ­¢è¡€å’Œå‡è¡€è¿‡ç¨‹ä¸­èµ·å…³é”®ä½œç”¨ã€‚æ•°é‡å¼‚å¸¸å¯èƒ½å½±å“å‡è¡€åŠŸèƒ½ã€‚"
            tips.append(f"- {tip}")
            report_lines.append(f"- {tip}")

    disclaimer = "é‡è¦æç¤ºï¼šæœ¬åˆ†æåŸºäºå½“å‰è§†é‡å†…çš„ç»†èƒè¯†åˆ«å’Œè®¡æ•°ï¼Œç»“æœ**é«˜åº¦ä¾èµ–äºå›¾åƒè´¨é‡å’Œä»£è¡¨æ€§**ï¼Œ**ä¸èƒ½**æ›¿ä»£æ ‡å‡†åŒ–çš„ä¸´åºŠè¡€æ¶²æ£€éªŒï¼ˆå¦‚å…¨è¡€ç»†èƒè®¡æ•° CBCï¼‰å’Œä¸“ä¸šåŒ»ç”Ÿè¯Šæ–­ã€‚ç»†èƒçš„ç»å¯¹æ•°é‡ã€ä½“ç§¯ã€åˆ†ç±»å’Œå½¢æ€å­¦è¯„ä¼°éœ€è¦å®éªŒå®¤ä¸“ä¸šæ£€æµ‹ã€‚è¯·åŠ¡å¿…å’¨è¯¢åŒ»ç”Ÿä»¥è·å–å‡†ç¡®çš„å¥åº·è¯„ä¼°ã€‚"
    tips.append(f"- {disclaimer}")
    report_lines.append("")
    report_lines.append(disclaimer)
    report_lines.append("")
    report_lines.append("--- æŠ¥å‘Šç»“æŸ ---")

    return tips, "\n".join(report_lines)

# --- åˆ›å»ºä¸‹è½½ ZIP æ–‡ä»¶ --- #
def create_download_zip(image_np, counts, report_text):
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:
        # 1. æ·»åŠ åˆ†ææŠ¥å‘Šæ–‡æœ¬
        zip_file.writestr('analysis_report.txt', report_text)

        # 2. æ·»åŠ æ ‡æ³¨åçš„å›¾åƒ
        # å°† NumPy å›¾åƒè½¬ä¸º PNG æ ¼å¼çš„å­—èŠ‚æµ
        is_success, buffer = cv2.imencode(".png", image_np)
        if is_success:
            image_bytes = io.BytesIO(buffer)
            zip_file.writestr('annotated_image.png', image_bytes.getvalue())
        else:
            # å¦‚æœå›¾åƒç¼–ç å¤±è´¥ï¼Œå¯ä»¥å†™å…¥ä¸€ä¸ªé”™è¯¯ä¿¡æ¯æˆ–è·³è¿‡
            zip_file.writestr('image_error.txt', 'æ— æ³•ç¼–ç æ ‡æ³¨åçš„å›¾åƒã€‚')

    zip_buffer.seek(0)
    return zip_buffer

# --- æ–°å¢ï¼šè°ƒç”¨ Ollama API çš„å‡½æ•° ---
def call_ollama_api(prompt_text):
    """è°ƒç”¨æœ¬åœ° Ollama API è·å–å¤§æ¨¡å‹åˆ†æ"""
    full_url = OLLAMA_BASE_URL + GENERATE_ENDPOINT
    payload = {
        "model": OLLAMA_MODEL_NAME,
        "prompt": prompt_text,
        "stream": False  # è®¾ç½®ä¸º False ä»¥è·å–å®Œæ•´å“åº”
    }
    try:
        response = requests.post(full_url, json=payload, timeout=120) # è®¾ç½®è¶…æ—¶
        response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥åˆ™æŠ›å‡º HTTPError
        # Ollama è¿”å›çš„ JSON ä¸­ï¼Œå®é™…çš„æ–‡æœ¬åœ¨ 'response' å­—æ®µ
        return response.json().get("response", "æœªèƒ½ä» Ollama è·å–æœ‰æ•ˆå“åº”ã€‚")
    except requests.exceptions.RequestException as e:
        st.error(f"è°ƒç”¨ Ollama API å¤±è´¥: {e}")
        st.warning("è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”æ¨¡å‹ '{OLLAMA_MODEL_NAME}' å·²ä¸‹è½½ã€‚æ‚¨å¯ä»¥é€šè¿‡å‘½ä»¤ `ollama pull {OLLAMA_MODEL_NAME}` æ¥ä¸‹è½½æ¨¡å‹ã€‚")
        return None
# ------------------------------------

# --- è¾“å…¥å¤„ç†é€»è¾‘ ---
st.subheader("ğŸ–¼ï¸ è¾“å…¥é¢„è§ˆ")
if input_method == 'ä¸Šä¼ å›¾åƒ':
    uploaded_image = st.file_uploader("é€‰æ‹©ä¸€ä¸ªå›¾åƒæ–‡ä»¶ (JPG, PNG)", type=["jpg", "png"], key="image_uploader")
    if uploaded_image is not None:
        # ä¿å­˜åŸå§‹å›¾åƒç”¨äºåç»­æ˜¾ç¤º
        image_bytes = uploaded_image.read()
        pil_image = Image.open(io.BytesIO(image_bytes))
        img_np = np.array(pil_image)
        if len(img_np.shape) == 3 and img_np.shape[2] == 4:
            img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2RGB)
        elif len(img_np.shape) == 2:
            img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)
        
        # ä¿å­˜åŸå§‹å›¾åƒåˆ°session state
        st.session_state['original_image'] = img_np
        
        if model:
            # ä½¿ç”¨å·²ç»å¤„ç†è¿‡çš„å›¾åƒï¼Œä¸éœ€è¦é‡å¤è¯»å–å’Œå¤„ç†

            results = model.predict(img_np, conf=confidence_threshold, iou=iou_threshold)
            annotated_img = results[0].plot()
            annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)

            counts = {'WBC': 0, 'RBC': 0, 'Platelets': 0}
            class_names = model.names
            detected_boxes = results[0].boxes
            if detected_boxes is not None and len(detected_boxes) > 0:
                detected_classes = detected_boxes.cls.cpu().numpy().astype(int)
                for i, cls_index in enumerate(detected_classes):
                    class_name = class_names.get(cls_index, f'æœªçŸ¥ç±»åˆ«_{cls_index}')
                    counts[class_name] = counts.get(class_name, 0) + 1
            else:
                st.write("æœªæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡ã€‚")

            st.session_state['annotated_image'] = annotated_img_rgb
            st.session_state['cell_counts'] = counts
            st.session_state['input_type'] = 'image' # Mark input type
            # Generate analysis text immediately after detection for image
            _, report_text = provide_health_tips(counts)
            st.session_state['report_text'] = report_text
            st.info("å›¾åƒæ£€æµ‹å®Œæˆï¼ç»“æœè§ä¸‹æ–¹ã€‚")
        else:
            st.error("æ— æ³•æ‰§è¡Œæ£€æµ‹ï¼Œå› ä¸ºæ¨¡å‹åŠ è½½å¤±è´¥ã€‚")
    else:
        if 'annotated_image' in st.session_state: del st.session_state['annotated_image']
        if 'cell_counts' in st.session_state: del st.session_state['cell_counts']
        if 'input_type' in st.session_state: del st.session_state['input_type']

elif input_method == 'ä¸Šä¼ è§†é¢‘':
    uploaded_video = st.file_uploader("é€‰æ‹©ä¸€ä¸ªè§†é¢‘æ–‡ä»¶", type=["mp4", "avi", "mov"], key="video_uploader")
    if uploaded_video is not None:
        st.video(uploaded_video)
        st.info("è§†é¢‘ä¸Šä¼ æˆåŠŸï¼Œç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹å¤„ç†...")
        process_video_button = st.button("å¤„ç†è§†é¢‘")

        if process_video_button and model:
            with st.spinner('æ­£åœ¨å¤„ç†è§†é¢‘ï¼Œè¯·ç¨å€™...'):
                tfile = tempfile.NamedTemporaryFile(delete=False)
                tfile.write(uploaded_video.read())
                video_path = tfile.name
                tfile.close()

                cap = cv2.VideoCapture(video_path)
                if not cap.isOpened():
                    st.error(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
                    if os.path.exists(video_path): os.unlink(video_path)
                else:
                    stframe = st.empty()
                    frame_count = 0
                    total_counts = {'WBC': 0, 'RBC': 0, 'Platelets': 0}
                    class_names = model.names
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                    if total_frames <= 0: st.warning("æ— æ³•è·å–è§†é¢‘æ€»å¸§æ•°ï¼Œè¿›åº¦æ¡å¯èƒ½ä¸å‡†ç¡®ã€‚")

                    while True:
                        ret, frame = cap.read()
                        if not ret: break
                        frame_count += 1
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        results = model.predict(frame_rgb, conf=confidence_threshold, iou=iou_threshold, verbose=False)
                        annotated_frame = results[0].plot()
                        detected_boxes = results[0].boxes
                        if detected_boxes is not None and len(detected_boxes) > 0:
                            detected_classes = detected_boxes.cls.cpu().numpy().astype(int)
                            for cls_index in detected_classes:
                                class_name = class_names.get(cls_index, f'æœªçŸ¥ç±»åˆ«_{cls_index}')
                                total_counts[class_name] = total_counts.get(class_name, 0) + 1

                        annotated_frame_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
                        caption_text = f'å¤„ç†ä¸­ - å¸§ {frame_count}'
                        if total_frames > 0: caption_text += f'/{total_frames}'
                        stframe.image(annotated_frame_rgb, caption=caption_text, use_container_width=True)
                        if total_frames > 0:
                            progress = frame_count / total_frames
                            progress_bar.progress(progress)
                            status_text.text(f"å¤„ç†è¿›åº¦: {int(progress * 100)}%")

                    cap.release()
                    if os.path.exists(video_path): os.unlink(video_path)
                    if total_frames > 0: progress_bar.progress(1.0)
                    status_text.text(f"è§†é¢‘å¤„ç†å®Œæˆï¼å…±å¤„ç† {frame_count} å¸§ã€‚")
                    st.success("è§†é¢‘å¤„ç†å®Œæˆï¼ç»“æœè§ä¸‹æ–¹ã€‚")
                    st.session_state['video_processed'] = True
                    st.session_state['video_total_counts'] = total_counts
                    st.session_state['input_type'] = 'video' # Mark input type
        elif process_video_button and not model:
             st.error("æ— æ³•æ‰§è¡Œæ£€æµ‹ï¼Œå› ä¸ºæ¨¡å‹åŠ è½½å¤±è´¥ã€‚")
    else:
        if 'video_processed' in st.session_state: del st.session_state['video_processed']
        if 'video_total_counts' in st.session_state: del st.session_state['video_total_counts']
        if 'input_type' in st.session_state: del st.session_state['input_type']

elif input_method == 'ä½¿ç”¨æ‘„åƒå¤´':
    st.info("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¯åŠ¨/åœæ­¢æ‘„åƒå¤´æ£€æµ‹ã€‚")
    col_cam_btn1, col_cam_btn2 = st.columns(2)
    start_button = col_cam_btn1.button('å¯åŠ¨æ‘„åƒå¤´', key='start_cam')
    stop_button = col_cam_btn2.button('åœæ­¢æ£€æµ‹', key='stop_cam')
    stframe_cam = st.empty()

    if 'cam_running' not in st.session_state:
        st.session_state['cam_running'] = False
    if 'cam_total_counts' not in st.session_state:
        st.session_state['cam_total_counts'] = {'WBC': 0, 'RBC': 0, 'Platelets': 0}
        st.session_state['cam_running'] = False

    if start_button:
        st.session_state['cam_running'] = True
        # Reset counts when starting
        st.session_state['cam_total_counts'] = {'WBC': 0, 'RBC': 0, 'Platelets': 0}
        st.session_state['cam_running'] = True
    if stop_button:
        st.session_state['cam_running'] = False
        stframe_cam.empty()
        # Generate report when stopping camera
        if 'cam_total_counts' in st.session_state:
            _, report_text_cam = provide_health_tips(st.session_state['cam_total_counts'])
            st.session_state['report_text_cam'] = report_text_cam
        st.info("æ‘„åƒå¤´æ£€æµ‹å·²åœæ­¢ã€‚")

    if st.session_state['cam_running']:
        if model:
            # Initialize camera only when running
            if 'cap_cam' not in st.session_state or st.session_state['cap_cam'] is None:
                st.session_state['cap_cam'] = cv2.VideoCapture(0)
                if not st.session_state['cap_cam'].isOpened():
                    st.error("æ— æ³•æ‰“å¼€æ‘„åƒå¤´ã€‚è¯·ç¡®ä¿æ‘„åƒå¤´å·²è¿æ¥å¹¶æˆæƒè®¿é—®ã€‚")
                    st.session_state['cam_running'] = False # Stop if cannot open
                    st.session_state['cap_cam'] = None
                else:
                     st.info("æ‘„åƒå¤´å·²å¯åŠ¨ï¼Œæ­£åœ¨è¿›è¡Œå®æ—¶æ£€æµ‹...")

            cap_cam = st.session_state.get('cap_cam')
            if cap_cam and cap_cam.isOpened():
                ret_cam, frame_cam = cap_cam.read()
                if not ret_cam:
                    st.warning("æ— æ³•ä»æ‘„åƒå¤´è¯»å–å¸§ã€‚")
                    st.session_state['cam_running'] = False # Stop if reading fails
                else:
                    frame_rgb_cam = cv2.cvtColor(frame_cam, cv2.COLOR_BGR2RGB)
                    results_cam = model.predict(frame_rgb_cam, conf=confidence_threshold, iou=iou_threshold, verbose=False)
                    annotated_frame_cam = results_cam[0].plot()
                    annotated_frame_rgb_cam = cv2.cvtColor(annotated_frame_cam, cv2.COLOR_BGR2RGB)
                    # Update counts for camera
                    detected_boxes_cam = results_cam[0].boxes
                    if detected_boxes_cam is not None and len(detected_boxes_cam) > 0:
                        detected_classes_cam = detected_boxes_cam.cls.cpu().numpy().astype(int)
                        class_names_cam = model.names
                        for cls_index_cam in detected_classes_cam:
                            class_name_cam = class_names_cam.get(cls_index_cam, f'æœªçŸ¥ç±»åˆ«_{cls_index_cam}')
                            st.session_state['cam_total_counts'][class_name_cam] = st.session_state['cam_total_counts'].get(class_name_cam, 0) + 1

                    stframe_cam.image(annotated_frame_rgb_cam, caption='å®æ—¶æ£€æµ‹', use_container_width=True)
                    # Trigger rerun to process next frame
                    st.rerun()
        else:
            st.error("æ— æ³•å¯åŠ¨æ‘„åƒå¤´æ£€æµ‹ï¼Œå› ä¸ºæ¨¡å‹åŠ è½½å¤±è´¥ã€‚")
            st.session_state['cam_running'] = False
    else:
         # Release camera when not running
         if 'cap_cam' in st.session_state and st.session_state['cap_cam'] is not None:
             st.session_state['cap_cam'].release()
             st.session_state['cap_cam'] = None
         # Don't clear report text here, keep it for display after stop

# --- æ˜¾ç¤ºç»“æœ ---
st.subheader("ğŸ“Š æ£€æµ‹ç»“æœä¸åˆ†æ")

current_input_type = st.session_state.get('input_type', None)

if current_input_type == 'image' and 'annotated_image' in st.session_state:
    original_image = st.session_state['original_image']
    annotated_image = st.session_state['annotated_image']
    counts = st.session_state.get('cell_counts', {})
    report_text_internal = st.session_state.get('report_text', "æŠ¥å‘Šç”Ÿæˆå¤±è´¥ã€‚") # BCDetection-X å†…éƒ¨æŠ¥å‘Š

    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ï¼Œå·¦ä¾§æ˜¾ç¤ºåŸå§‹å›¾åƒï¼Œå³ä¾§æ˜¾ç¤ºæ£€æµ‹ç»“æœ
    col1, col2 = st.columns(2)
    
    with col1:
        st.image(original_image, caption='åŸå§‹å›¾åƒ', use_container_width=True)
    
    with col2:
        st.image(annotated_image, caption='æ£€æµ‹ç»“æœ', use_container_width=True)
    st.write("**ç»†èƒè®¡æ•°:**")
    if counts:
        for cell_type, count in counts.items():
            st.write(f"- {cell_type}: {count}")

        # --- æ·»åŠ å›¾è¡¨ --- #
        st.subheader("ç»†èƒæ•°é‡åˆ†å¸ƒ")
        df_counts = pd.DataFrame(list(counts.items()), columns=['ç»†èƒç±»å‹', 'æ•°é‡'])
        if not df_counts.empty:
            chart = alt.Chart(df_counts).mark_bar().encode(
                x=alt.X('ç»†èƒç±»å‹', sort='-y'), # Sort bars by count
                y='æ•°é‡',
                color='ç»†èƒç±»å‹',
                tooltip=['ç»†èƒç±»å‹', 'æ•°é‡']
            ).interactive()
            st.altair_chart(chart, use_container_width=True)
        else:
            st.write("æ— æ•°æ®æ˜¾ç¤ºã€‚")

        # --- å¥åº·é£é™©æç¤º --- #
        st.subheader("âš ï¸ åˆæ­¥åˆ†æä¸æç¤º (BCDetection-X)")
        health_tips_display, _ = provide_health_tips(counts) # provide_health_tips è¿”å› (tips_for_display, full_report_text)
        if health_tips_display:
            for tip in health_tips_display:
                st.warning(tip)
        
        # --- æ–°å¢ï¼šOllama AI åˆ†ææŒ‰é’®å’Œç»“æœå±•ç¤º ---
        st.markdown("---")
        st.subheader(f"ğŸ¤– AI å¤§æ¨¡å‹æ·±åº¦åˆ†æ (æ¨¡å‹: BCDetection-X AI)")
        if st.button("è·å– AI è¿›ä¸€æ­¥åˆ†æå»ºè®®", key="ollama_image_analysis"):
            with st.spinner(f"æ­£åœ¨è¯·æ±‚ BCDetection-X AI è¿›è¡Œåˆ†æï¼Œè¯·ç¨å€™..."):
                # æ„å»ºç»™ Ollama çš„ Prompt
                prompt_to_ollama = f"""
                ä¸€åä¸“ä¸šçš„è¡€æ¶²å­¦AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹è¡€ç»†èƒåˆæ­¥æ£€æµ‹ç»“æœå’Œåˆ†æï¼Œæä¾›æ›´æ·±å±‚æ¬¡çš„è§£è¯»ã€å¯èƒ½çš„ç—…å› åˆ†æä»¥åŠç›¸å…³çš„å¥åº·å»ºè®®ã€‚

                [åˆæ­¥æ£€æµ‹æ•°æ®]
                ç»†èƒè®¡æ•°:
                - ç™½ç»†èƒ (WBC): {counts.get('WBC', 0)}
                - çº¢ç»†èƒ (RBC): {counts.get('RBC', 0)}
                - è¡€å°æ¿ (Platelets): {counts.get('Platelets', 0)}
                åˆæ­¥åˆ†æä¸æç¤º (æ¥è‡ª BCDetection-X ç³»ç»Ÿ):
                {report_text_internal}

                [ä½ çš„ä»»åŠ¡]
                1.  åŸºäºä¸Šè¿°æ•°æ®ï¼Œè¯¦ç»†åˆ†æå„ç§ç»†èƒè®¡æ•°çš„ä¸´åºŠæ„ä¹‰ã€‚
                2.  æ¢è®¨è¿™äº›æ•°æ®ç»„åˆå¯èƒ½æŒ‡å‘çš„æ½œåœ¨å¥åº·é—®é¢˜æˆ–ç–¾ç—…é£é™©ï¼ˆä¾‹å¦‚ï¼Œè´«è¡€ã€æ„ŸæŸ“ã€ç‚ç—‡ã€å‡è¡€åŠŸèƒ½å¼‚å¸¸ç­‰ï¼‰ã€‚
                3.  å¦‚æœæ•°æ®å­˜åœ¨å¼‚å¸¸ï¼Œè¯·æ¨æµ‹å¯èƒ½çš„ç—…å› ã€‚
                4.  æä¾›ä¸€äº›å¸¸è§„çš„å¥åº·ç®¡ç†å»ºè®®æˆ–åç»­æ£€æŸ¥å»ºè®®ã€‚
                5.  è¯·ç”¨æ¸…æ™°ã€æ˜“æ‡‚çš„è¯­è¨€è¿›è¡Œè§£é‡Šï¼Œå¹¶åˆ†ç‚¹é˜è¿°ã€‚
                6.  åœ¨å›ç­”çš„æœ€åï¼Œè¯·åŠ¡å¿…å¼ºè°ƒï¼šâ€œä»¥ä¸Šåˆ†æä»…ä¸ºAIæ¨¡å‹æ ¹æ®æä¾›æ•°æ®è¿›è¡Œçš„æ¨æµ‹ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­ã€‚å¦‚æœ‰ä»»ä½•å¥åº·ç–‘è™‘ï¼Œè¯·åŠ¡å¿…å’¨è¯¢æ‰§ä¸šåŒ»å¸ˆã€‚â€

                è¯·å¼€å§‹ä½ çš„åˆ†æï¼š
                """
                ollama_response = call_ollama_api(prompt_to_ollama)
                if ollama_response:
                    st.session_state['ollama_image_report'] = ollama_response
                else:
                    st.session_state['ollama_image_report'] = "AI åˆ†æå¤±è´¥æˆ–æ— å“åº”ã€‚"
        
        if 'ollama_image_report' in st.session_state:
            with st.expander("æŸ¥çœ‹ AI å¤§æ¨¡å‹åˆ†æç»“æœ", expanded=True):
                st.markdown(st.session_state['ollama_image_report'])
        # ------------------------------------------

        # --- ä¸‹è½½æŒ‰é’® --- #
        # ... (ä¸‹è½½æŒ‰é’®ä»£ç ä¿æŒä¸å˜, ä½†å¯ä»¥è€ƒè™‘å°† Ollama æŠ¥å‘Šä¹ŸåŠ å…¥ ZIP) ...

    else:
        st.write("æœªæ£€æµ‹åˆ°ç¬¦åˆæ¡ä»¶çš„ç»†èƒã€‚")

elif current_input_type == 'video' and st.session_state.get('video_processed', False):
    st.write("**è§†é¢‘å¤„ç†å®Œæˆã€‚**")
    st.write("**æ€»ç»†èƒè®¡æ•° (ä¼°ç®—):**")
    total_counts_video = st.session_state.get('video_total_counts', {})
    # å‡è®¾è§†é¢‘ä¹Ÿæœ‰ä¸€ä¸ªåˆæ­¥æŠ¥å‘Šæ–‡æœ¬ï¼Œå¦‚æœä¹‹å‰æ²¡æœ‰ç”Ÿæˆï¼Œè¿™é‡Œéœ€è¦ç”Ÿæˆæˆ–è°ƒæ•´
    # _, report_text_video_internal = provide_health_tips(total_counts_video) # ç¤ºä¾‹
    # st.session_state['report_text_video_internal'] = report_text_video_internal

    if total_counts_video:
        for cell_type, count in total_counts_video.items():
             st.write(f"- {cell_type}: {count}")

        # ... (è§†é¢‘çš„å›¾è¡¨å’Œåˆæ­¥å¥åº·æç¤ºä»£ç ä¿æŒä¸å˜) ...
        
        # --- æ–°å¢ï¼šOllama AI åˆ†ææŒ‰é’®å’Œç»“æœå±•ç¤º (è§†é¢‘) ---
        st.markdown("---")
        st.subheader(f"ğŸ¤– AI å¤§æ¨¡å‹æ·±åº¦åˆ†æ (æ¨¡å‹: {OLLAMA_MODEL_NAME})")
        if st.button("è·å– AI è¿›ä¸€æ­¥åˆ†æå»ºè®® (è§†é¢‘)", key="ollama_video_analysis"):
            with st.spinner(f"æ­£åœ¨è¯·æ±‚ BCDetection-X AI è¿›è¡Œåˆ†æï¼Œè¯·ç¨å€™..."):
                # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å‡è®¾è§†é¢‘ä¹Ÿæœ‰ä¸€ä¸ª report_text_internal
                # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‚¨å¯èƒ½éœ€è¦ä¸ºè§†é¢‘ç»“æœå•ç‹¬ç”Ÿæˆæˆ–æ±‡æ€»ä¸€ä¸ªåˆæ­¥æŠ¥å‘Š
                video_preliminary_report = "è§†é¢‘åˆæ­¥åˆ†ææ‘˜è¦ï¼š\n" + \
                                           f"- ç™½ç»†èƒ (WBC) æ€»è®¡: {total_counts_video.get('WBC', 0)}\n" + \
                                           f"- çº¢ç»†èƒ (RBC) æ€»è®¡: {total_counts_video.get('RBC', 0)}\n" + \
                                           f"- è¡€å°æ¿ (Platelets) æ€»è®¡: {total_counts_video.get('Platelets', 0)}\n" + \
                                           "è¯·æ³¨æ„ï¼Œè§†é¢‘ç»“æœä¸ºå¤šå¸§ç´¯ç§¯ä¼°ç®—ã€‚"

                prompt_to_ollama_video = f"""
                ä¸€åä¸“ä¸šçš„è¡€æ¶²å­¦AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹è§†é¢‘ä¸­è¡€ç»†èƒçš„ç´¯ç§¯ä¼°ç®—ç»“æœå’Œåˆ†æï¼Œæä¾›æ›´æ·±å±‚æ¬¡çš„è§£è¯»ã€å¯èƒ½çš„ç—…å› åˆ†æä»¥åŠç›¸å…³çš„å¥åº·å»ºè®®ã€‚

                [åˆæ­¥æ£€æµ‹æ•°æ® - è§†é¢‘ä¼°ç®—]
                ç»†èƒæ€»è®¡æ•° (ä¼°ç®—):
                - ç™½ç»†èƒ (WBC): {total_counts_video.get('WBC', 0)}
                - çº¢ç»†èƒ (RBC): {total_counts_video.get('RBC', 0)}
                - è¡€å°æ¿ (Platelets): {total_counts_video.get('Platelets', 0)}
                åˆæ­¥åˆ†æä¸æç¤º (æ¥è‡ª BCDetection-X ç³»ç»Ÿ - è§†é¢‘æ‘˜è¦):
                {video_preliminary_report} 

                [ä½ çš„ä»»åŠ¡]
                1.  åŸºäºä¸Šè¿°ç´¯ç§¯ä¼°ç®—æ•°æ®ï¼Œåˆ†æå„ç§ç»†èƒè®¡æ•°çš„ä¸´åºŠæ„ä¹‰ã€‚
                2.  æ¢è®¨è¿™äº›æ•°æ®ç»„åˆå¯èƒ½æŒ‡å‘çš„æ½œåœ¨å¥åº·é—®é¢˜æˆ–ç–¾ç—…é£é™©ã€‚
                3.  å¦‚æœæ•°æ®å­˜åœ¨å¼‚å¸¸ï¼Œè¯·æ¨æµ‹å¯èƒ½çš„ç—…å› ã€‚
                4.  æä¾›ä¸€äº›å¸¸è§„çš„å¥åº·ç®¡ç†å»ºè®®æˆ–åç»­æ£€æŸ¥å»ºè®®ã€‚
                5.  è¯·ç”¨æ¸…æ™°ã€æ˜“æ‡‚çš„è¯­è¨€è¿›è¡Œè§£é‡Šï¼Œå¹¶åˆ†ç‚¹é˜è¿°ã€‚
                6.  åœ¨å›ç­”çš„æœ€åï¼Œè¯·åŠ¡å¿…å¼ºè°ƒï¼šâ€œä»¥ä¸Šåˆ†æä»…ä¸ºAIæ¨¡å‹æ ¹æ®æä¾›æ•°æ®è¿›è¡Œçš„æ¨æµ‹ï¼Œä¸”åŸºäºè§†é¢‘å¤šå¸§ä¼°ç®—ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­ã€‚å¦‚æœ‰ä»»ä½•å¥åº·ç–‘è™‘ï¼Œè¯·åŠ¡å¿…å’¨è¯¢æ‰§ä¸šåŒ»å¸ˆã€‚â€

                è¯·å¼€å§‹ä½ çš„åˆ†æï¼š
                """
                ollama_response_video = call_ollama_api(prompt_to_ollama_video)
                if ollama_response_video:
                    st.session_state['ollama_video_report'] = ollama_response_video
                else:
                    st.session_state['ollama_video_report'] = "AI åˆ†æå¤±è´¥æˆ–æ— å“åº”ã€‚"
            
            if 'ollama_video_report' in st.session_state:
                with st.expander("æŸ¥çœ‹ AI å¤§æ¨¡å‹åˆ†æç»“æœ (è§†é¢‘)", expanded=True):
                    st.markdown(st.session_state['ollama_video_report'])
            # ------------------------------------------

            # --- ä¸‹è½½æŒ‰é’® (æ–‡æœ¬æŠ¥å‘Š) --- #
            # ... (ä¸‹è½½æŒ‰é’®ä»£ç ä¿æŒä¸å˜) ...
        else:
             st.write("åœ¨è§†é¢‘ä¸­æœªæ£€æµ‹åˆ°ç¬¦åˆæ¡ä»¶çš„ç»†èƒã€‚")

    elif input_method == 'ä½¿ç”¨æ‘„åƒå¤´':
        if st.session_state.get('cam_running', False):
            st.info("å®æ—¶æ£€æµ‹ç»“æœæ˜¾ç¤ºåœ¨å·¦ä¾§ï¼Œç»Ÿè®¡å›¾è¡¨åœ¨ä¸‹æ–¹å®æ—¶æ›´æ–°ã€‚")
            st.subheader("å®æ—¶ç»†èƒè®¡æ•° (ç´¯ç§¯)")
            cam_counts = st.session_state.get('cam_total_counts', {})
            if cam_counts and sum(cam_counts.values()) > 0:
                df_counts_cam = pd.DataFrame(list(cam_counts.items()), columns=['ç»†èƒç±»å‹', 'æ•°é‡'])
                chart_cam = alt.Chart(df_counts_cam).mark_bar().encode(
                    x=alt.X('ç»†èƒç±»å‹', sort='-y'),
                    y='æ•°é‡',
                    color='ç»†èƒç±»å‹',
                    tooltip=['ç»†èƒç±»å‹', 'æ•°é‡']
                ).interactive()
                st.altair_chart(chart_cam, use_container_width=True)
            else:
                st.write("æ‘„åƒå¤´è¿è¡Œä¸­ï¼Œæš‚æœªæ£€æµ‹åˆ°ç»†èƒæˆ–è®¡æ•°ä¸ºé›¶ã€‚")
        else:
            # Display final counts after stopping
            st.info("æ‘„åƒå¤´å·²åœæ­¢ã€‚æ˜¾ç¤ºæœ€ç»ˆç´¯ç§¯è®¡æ•°ï¼š")
            final_cam_counts = st.session_state.get('cam_total_counts', {})
            report_text_cam = st.session_state.get('report_text_cam', "æŠ¥å‘Šç”Ÿæˆå¤±è´¥ã€‚")

            if final_cam_counts and sum(final_cam_counts.values()) > 0:
                st.write("**æœ€ç»ˆç»†èƒè®¡æ•° (æ‘„åƒå¤´):**")
                for cell_type, count in final_cam_counts.items():
                    st.write(f"- {cell_type}: {count}")

                st.subheader("æœ€ç»ˆç»†èƒæ•°é‡åˆ†å¸ƒ (æ‘„åƒå¤´)")
                df_final_counts_cam = pd.DataFrame(list(final_cam_counts.items()), columns=['ç»†èƒç±»å‹', 'æ•°é‡'])
                chart_final_cam = alt.Chart(df_final_counts_cam).mark_bar().encode(
                    x=alt.X('ç»†èƒç±»å‹', sort='-y'),
                    y='æ•°é‡',
                    color='ç»†èƒç±»å‹',
                    tooltip=['ç»†èƒç±»å‹', 'æ•°é‡']
                ).interactive()
                st.altair_chart(chart_final_cam, use_container_width=True)

                # Health tips based on final count
                st.subheader("âš ï¸ åˆæ­¥åˆ†æä¸æç¤º (åŸºäºæ‘„åƒå¤´ç´¯ç§¯è®¡æ•°ï¼Œä»…ä¾›å‚è€ƒ)")
                health_tips_cam, _ = provide_health_tips(final_cam_counts)
                if health_tips_cam:
                    for tip in health_tips_cam:
                        st.warning(tip)
                # st.caption("å…è´£å£°æ˜: æœ¬ç³»ç»Ÿç»“æœä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­ã€‚")
            else:
                st.write("æ‘„åƒå¤´è¿è¡Œæ—¶æœªæ£€æµ‹åˆ°ç»†èƒï¼Œæˆ–æœ€ç»ˆè®¡æ•°ä¸ºé›¶ã€‚")
        # Potential place for cumulative stats if implemented for camera

    else:
        st.info("è¯·å…ˆé€‰æ‹©è¾“å…¥æºå¹¶ä¸Šä¼ æ–‡ä»¶æˆ–å¯åŠ¨æ‘„åƒå¤´ä»¥æŸ¥çœ‹ç»“æœã€‚")