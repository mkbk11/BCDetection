import streamlit as st
import requests
import json # For pretty printing JSON if needed

# --- Ollama API é…ç½® ---
OLLAMA_BASE_URL = "http://127.0.0.1:11434"
GENERATE_ENDPOINT = "/api/generate"
MODEL_NAME = "deepseek-r1:7b" # ç¡®ä¿ Ollama æœåŠ¡ä¸­å·²æ‹‰å–æ­¤æ¨¡å‹
# -------------------------

st.set_page_config(page_title="AI åŠ©æ‰‹", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ¤– BCDetection-X - AI åŠ©æ‰‹")
st.caption(f"ç”± BCDetection-AI æ¨¡å‹é©±åŠ¨")

st.markdown("--- ")
# st.markdown("æ¬¢è¿ä½¿ç”¨ AI åŠ©æ‰‹ï¼æ‚¨å¯ä»¥è¾“å…¥ä»»ä½•ä¸åŒ»å­¦ã€å¥åº·ã€ç–¾ç—…è¯Šç–—ç›¸å…³çš„é—®é¢˜ï¼ŒAI å°†å°½åŠ›ä¸ºæ‚¨æä¾›ä¿¡æ¯å’Œè§£ç­”ã€‚")
st.markdown("æ¬¢è¿ä½¿ç”¨ AI åŠ©æ‰‹ï¼AI å°†å°½åŠ›ä¸ºæ‚¨æä¾›ä¿¡æ¯å’Œè§£ç­”ã€‚")

# --- ä¼šè¯çŠ¶æ€åˆå§‹åŒ– ---
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'current_model' not in st.session_state:
    st.session_state.current_model = MODEL_NAME

# --- æ¨¡å‹é€‰æ‹© (å¯é€‰) ---
# st.sidebar.header("æ¨¡å‹è®¾ç½®")
# available_models = ["deepseek-r1:7b", "qwen:7b", "llama3"] # ç¤ºä¾‹æ¨¡å‹åˆ—è¡¨
# selected_model = st.sidebar.selectbox("é€‰æ‹©è¯­è¨€æ¨¡å‹:", available_models, index=available_models.index(st.session_state.current_model) if st.session_state.current_model in available_models else 0)
# if selected_model != st.session_state.current_model:
#     st.session_state.current_model = selected_model
#     st.session_state.chat_history = [] # æ¨¡å‹æ›´æ”¹æ—¶æ¸…ç©ºå†å²è®°å½•
#     st.rerun()

# --- API è°ƒç”¨å‡½æ•° ---
def call_ollama_api(prompt_text, model_name):
    full_url = OLLAMA_BASE_URL + GENERATE_ENDPOINT
    payload = {
        "model": model_name,
        "prompt": prompt_text,
        "stream": False  # è®¾ç½®ä¸º False ä»¥è·å–å®Œæ•´å“åº”
    }
    try:
        response = requests.post(full_url, json=payload, timeout=180) # å¢åŠ è¶…æ—¶æ—¶é—´
        response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥åˆ™æŠ›å‡º HTTPError
        # Ollama è¿”å›çš„ JSON ä¸­ï¼Œå®é™…çš„æ–‡æœ¬åœ¨ 'response' å­—æ®µ
        return response.json().get("response", "æœªèƒ½ä» Ollama è·å–æœ‰æ•ˆå“åº”ã€‚")
    except requests.exceptions.Timeout:
        st.error(f"è°ƒç”¨ Ollama API è¶…æ—¶ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– Ollama æœåŠ¡çŠ¶æ€ã€‚")
        return None
    except requests.exceptions.ConnectionError:
        st.error(f"æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ ({OLLAMA_BASE_URL})ã€‚è¯·ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œã€‚")
        return None
    except requests.exceptions.RequestException as e:
        st.error(f"è°ƒç”¨ Ollama API å¤±è´¥: {e}")
        st.warning(f"è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”æ¨¡å‹ '{model_name}' å·²ä¸‹è½½ã€‚æ‚¨å¯ä»¥é€šè¿‡å‘½ä»¤ `ollama pull {model_name}` æ¥ä¸‹è½½æ¨¡å‹ã€‚")
        return None

# --- èŠå¤©ç•Œé¢ ---
# æ˜¾ç¤ºèŠå¤©å†å²
chat_container = st.container()
with chat_container:
    for i, chat in enumerate(st.session_state.chat_history):
        if chat["role"] == "user":
            st.chat_message("user", avatar="ğŸ§‘â€âš•ï¸").write(chat["content"])
        else:
            st.chat_message("assistant", avatar="ğŸ¤–").write(chat["content"])

# ç”¨æˆ·è¾“å…¥
# prompt = st.chat_input(f"å‘ {st.session_state.current_model} æé—®...")
prompt = st.chat_input(f"å‘ BCDetection-AI æé—®...")


if prompt:
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    with chat_container:
        st.chat_message("user", avatar="ğŸ§‘â€âš•ï¸").write(prompt)
    
    with st.spinner(f"AI ({st.session_state.current_model}) æ­£åœ¨æ€è€ƒä¸­..."):
        # æ„å»ºæ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ç»™æ¨¡å‹ (å¯é€‰ï¼Œä½†æ¨è)
        # full_prompt_to_model = "\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state.chat_history])
        # For simplicity, just send the last prompt, but for better conversation, send history.
        # For a more robust solution, you might want to manage the context window size.
        contextual_prompt = ""
        # ç®€å•çš„ä¸Šä¸‹æ–‡æ„å»ºï¼šåŒ…å«æœ€è¿‘å‡ è½®å¯¹è¯
        history_to_include = st.session_state.chat_history[-5:] # ä¾‹å¦‚ï¼ŒåŒ…å«æœ€è¿‘5æ¡æ¶ˆæ¯
        for msg in history_to_include:
            if msg["role"] == "user":
                contextual_prompt += f"ç”¨æˆ·: {msg['content']}\n"
            else:
                contextual_prompt += f"åŠ©æ‰‹: {msg['content']}\n"
        # ç¡®ä¿æœ€åæ˜¯å½“å‰ç”¨æˆ·çš„é—®é¢˜
        if not contextual_prompt.endswith(f"ç”¨æˆ·: {prompt}\n"):
             contextual_prompt += f"ç”¨æˆ·: {prompt}\n"
        contextual_prompt += "åŠ©æ‰‹: " # æç¤ºæ¨¡å‹ç»§ç»­å›ç­”

        ai_response = call_ollama_api(contextual_prompt, st.session_state.current_model)
        
    if ai_response:
        st.session_state.chat_history.append({"role": "assistant", "content": ai_response})
        with chat_container:
            st.chat_message("assistant", avatar="ğŸ¤–").write(ai_response)
    else:
        # å¦‚æœ API è°ƒç”¨å¤±è´¥ï¼Œä¹Ÿè®°å½•ä¸€ä¸ªé”™è¯¯ä¿¡æ¯åˆ°èŠå¤©å†å²ï¼Œå¹¶æ˜¾ç¤º
        error_message = "æŠ±æ­‰ï¼ŒAI åŠ©æ‰‹å½“å‰æ— æ³•å“åº”ã€‚è¯·ç¨åå†è¯•æˆ–æ£€æŸ¥ Ollama æœåŠ¡ã€‚"
        st.session_state.chat_history.append({"role": "assistant", "content": error_message})
        with chat_container:
            st.chat_message("assistant", avatar="ğŸ¤–").error(error_message)

# --- æ¸…ç©ºèŠå¤©è®°å½•æŒ‰é’® ---
if st.sidebar.button("æ¸…é™¤èŠå¤©è®°å½•"):
    st.session_state.chat_history = []
    st.rerun()

st.sidebar.markdown("--- ")
st.sidebar.info("æç¤ºï¼šæ­¤ AI åŠ©æ‰‹æä¾›çš„ä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®ã€‚")

# st.markdown("--- ")
# st.markdown("<p style='text-align: center; color: grey;'>ç”± èŠ¯èšåŠ›ç§‘æŠ€ æä¾›æŠ€æœ¯æ”¯æŒ</p>", unsafe_allow_html=True)